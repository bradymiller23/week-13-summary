---
title: "week 13 notes"
format: html
editor: 
  markdown: 
    wrap: 72
---

## Tuesday April 11

```{r}
packages <- c(
    # Old packages
    "ISLR2",
    "dplyr",
    "tidyr",
    "readr",
    "purrr",
    "repr",
    "tidyverse",
    "kableExtra",
    "IRdisplay",
    # NEW
    "torch",
    "torchvision",
    "luz"
)

# renv::install(packages)
sapply(packages, require, character.only=TRUE)
```


#### Application of neural networks to breast cancer & Titanic datasets
```{r}
url <- "https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv"

df <- read_csv(url) %>%
    mutate_if(\(x) is.character(x), as.factor) %>%
    mutate(y = Survived) %>%
    select(-c(Name, Survived)) %>%
    (\(x) {
        names(x) <- tolower(names(x))
        x
    })
```

```{r}
# url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data"

# col_names <- c("id", "diagnosis", paste0("feat", 1:30))

# df <- read_csv(
#         url, col_names, col_types = cols()
#     ) %>% 
#     select(-id) %>% 
#     mutate(y = ifelse(diagnosis == "M", 1, 0)) %>%
#     select(-diagnosis)


# df %>% head
```

```{r}
k <- 5

test_ind <- sample(
    1:nrow(df), 
    floor(nrow(df) / k),
    replace=FALSE
)

df_train <- df[-test_ind, ]
df_test  <- df[test_ind, ]

nrow(df_train) + nrow(df_test) == nrow(df)
```

```{r}
fit_glm <- glm(
    y ~ ., 
    df_train %>% mutate_at("y", factor), 
    family = binomial()
)

glm_test <- predict(
    fit_glm, 
    df_test,
    output = "response"
)

glm_preds <- ifelse(glm_test > 0.5, 1, 0)
table(glm_preds, df_test$y)
```

```{r}
NNet <- nn_module(
  initialize = function(p, q1, q2, q3) {  
    self$hidden1 <- nn_linear(p, q1)
    self$hidden2 <- nn_linear(q1, q2)
    self$hidden3 <- nn_linear(q2, q3)
    self$output <- nn_linear(q3, 1)
    self$activation <- nn_relu()
    self$sigmoid <- nn_sigmoid()
  },
    
  forward = function(x) {
    x %>% 
      self$hidden1() %>% self$activation() %>% 
      self$hidden2() %>% self$activation() %>% 
      self$hidden3() %>% self$activation() %>% 
      self$output() %>% self$sigmoid()
  }
)
```

```{r}
# fitting a model without an intercept - when its zero, everything else is zero
M <- model.matrix(y ~ 0 + ., data = df_train)
# model.matrix(y ~ ., data = df_train) [ ,-1]

fit_nn <- NNet %>%
    #
    # Setup the model
    #
    setup(
        loss = nn_bce_loss(),
        optimizer = optim_adam, 
        metrics = list(
            luz_metric_accuracy()
        )
    ) %>% 
    #
    # Set the hyperparameters
    #
    set_hparams(p=ncol(M), q1=256, q2=128, q3=64) %>% 
    set_opt_hparams(lr=0.005) %>% 
    #
    # Fit the model
    #
    fit(
        data = list(
            model.matrix(y ~ 0 + ., data = df_train),
            df_train %>% select(y) %>% as.matrix
        ),
        valid_data = list(
            model.matrix(y ~ 0 + ., data = df_test),
            df_test %>% select(y) %>% as.matrix
        ),
        epochs = 50, 
        verbose = TRUE
    )
```

```{r}
plot(fit_nn)
```

```{r}
nn_test <- predict(
    fit_nn, 
    model.matrix(y ~ . - 1, data = df_test)
)
# nn_test
nn_preds <- ifelse(nn_test > 0.5, 1, 0)

table(nn_preds, df_test$y)
```

```{r}
mean(nn_preds == df_test$y)
```

```{r}
table(glm_preds, df_test$y)
```


```{r}
mean(glm_preds == df_test$y)
```





#### DataLoaders

* key component in the ML pipeline
* handle loading and preprocessing of data in efficient way for training and 
  evaluating models
* make it easy to work with large datasets (ex. dataset with trillions of bits)
    * Loading data in smaller batches called chunks
    
* Advantages 

1. Efficiency memory management
1. Parallelism
1. Preprocessing
1. Flexibility
1. Standardization


* involves partitioning data into smaller chunks (batches)
* inside one large epoch of gradient descent, it'll break up the data into 
  batches and mini gradient descents (does gradient descent on every batch)
* IS DIFFERENT FROM CROSS VALIDATION
* training data is split up into smaller chunks - data is being cycled through
  during gradient descent 
* if batches are fixed statically (first 100 are batch 1, next 100 are batch 2),
  this is going to introduce some bias into the gradient descent process, so you
  want to shuffle the data between every epoch
* DataLoader takes care of these issues

```{r}
transform <- function(x) x %>%
  torch_tensor() %>%
  torch_flatten() %>%
  torch_div(255)


dir <- "./mnist"

train_ds <- mnist_dataset(
    root = dir,
    train = TRUE,
    download = TRUE,
    transform = transform
)
test_ds <- mnist_dataset(
    root = dir,
    train = FALSE,
    download = TRUE,
    transform = transform
)

```


```{r}
typeof(train_ds)
length(train_ds)
```

```{r}
# 42,000nd  observation is a 28x28 matrix
# one row is a 28x28 matrix
# is an image where a value specifies the intensity of the pixels in the 28x28 image
train_ds$data[42000, ,]
```
    


#### Using the data/DataLoaders to determine the value of a handwritten number

```{r}
# collection of {x_i, y_i} where i = 1, ranging to 10000
# x_i is 28x28 image of handwritten image
# every y_i {0,1,...,9}
options(repr.plot.width=10, repr.plot.height=10)

i <- sample(1:length(train_ds), 1)
x <- train_ds$data[i, ,] %>% t

image(x[1:28, 28:1], useRaster=TRUE, axes=FALSE, col=gray.colors(1000), main = train_ds$targets[i]-1 )
```
* data frame with each row being a single image observation and its actual value
* p (# cols) = # of pixels in width^2 so you can have column for each pixel



```{r}
# splitting up batches into size 128
# shuffling allows to negate any bias by shuffling observations for each epoch
train_dl <- dataloader(train_ds, batch_size = 1024, shuffle = TRUE)
test_dl <- dataloader(test_ds, batch_size = 1024)
```

```{r}
NNet_10 <- nn_module(
  initialize = function(p, q1, q2, q3, o) {
    self$hidden1 <- nn_linear(p, q1)
    self$hidden2 <- nn_linear(q1, q2)
    self$hidden3 <- nn_linear(q2, q3)
    self$OUTPUT <- nn_linear(q3, o)
    self$activation <- nn_relu()
  },
  forward = function(x) {
    x %>%
      self$hidden1() %>%
      self$activation() %>%
      self$hidden2() %>%
      self$activation() %>%
      self$hidden3() %>%
      self$activation() %>%
      self$OUTPUT()
  }
)
```



Unable to get this chunk to work as it gives error that says it can't convert
argument: input
```{r}
fit_nn <- NNet_10 %>%
    #
    # Setup the model
    #
    setup(
        loss = nn_cross_entropy_loss(),
        optimizer = optim_adam,
        metrics = list(
            luz_metric_accuracy()
        )
    ) %>%
    #
    # Set the hyperparameters
    #
    set_hparams(p=28*28, q1=256, q2=128, q3=64, o=10) %>% 
    #
    # Fit the model
    #
    fit(
        epochs = 10,
        data = train_dl,
        # valid_data = test_dl,
        verbose=TRUE
    )
```

```{r}
NN10_preds <- fit_nn %>% 
  predict(test_ds) %>% 
  torch_argmax(dim = 2) %>%
  as_array()
```

```{r}
mean(NN10_preds == test_ds$targets)
```

Table that displays what each digit is predicted as (correct predictions are on the diagonal)
```{r}
table(NN10_preds - 1, test_ds$targets - 1)
```

```{r}
caret::confusionMatrix(
  (NN10_preds - 1) %>% as.factor, 
  (test_ds$targets - 1) %>% as.factor
)
```





## Thursday, April 13


#### Supervised vs Unsupervised Learning


###### Supervised Learning

* when we have access to lablled data (given covariates and the responses)

* given this data our goal had been to predict y using $X_1, X_2, \dots, X_p$ as well as understand how each $X_i$ influences the response y

###### Unsupervised Learning

* Don't have access to labelled data (only given observations and the covariates for each)
* Don't know ground truth
* Goal is to identify interesting relationships between $X_1, X_2, \dots, X_p$ which can be done through...

1.  Dimension reduction

-   can we discover subgroups of variables $X_1, X_2, ..., X_p$, which
    behave similarly?
-   subsetting based on how similar they are

1.  clustering

-   can we discover subgroups of observations 1, 2, ..., n which behave
    similarly
-   do for observations


* Unsupervised learning is more 'popular' as it is easier to obtain unlabelled data than labeled data as this requires someone to assign the proper response variable


#### Principle Component Analysis

* PCA is one of the methods used to tackle the issue of dimension reduction

* objective - given variables($X_1, X_2, ..., X_p$), PCA produces a low dimension representation of the dataset (creating lower dimension dataset)

* it compresses data from each row/observation into 2 variables


Step 1

The first principle component $Z_1$ is the normalized linear combination of the features 
$$
Z_1 = v_{11}X_1 + v_{21}X_2 + \dots + v_{p1}X_p
$$
such that $Z_1$ has the largest possible variance and the sum of $v_{p,i}^2 = 1$


Step 2
The first principle component $Z_2$ is the normalized linear combination of the features
$$
Z_2 = v_{12}X_1 + v_{22}X_2 + \dots + v_{p2}X_p
$$
such that $V_2$ is orthogonal to $V_1$, $Z_2$ has the largest possible variance and the sum of $v_{p,2}^2 = 1$ 


* This step occurs up until the qth principal component $Z_q$ such that $Z_q$ has the largest possible variance $V_q$ is orthogonal to span($V_1, V_2, \dots, V_{q-1}$) and $v_{p,2}^2 = 1$  


#### Example of PCA

```{r}
data <- tibble(x1 = rnorm(100, mean = 0, sd = 1),
               x2 = x1 + rnorm(100, mean = 0, sd = 0.1)
               )

pca <- princomp(data, cor = TRUE)
summary(pca)
```

```{r}
pca$loadings
```
